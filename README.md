# ğŸ§  LLM Knowledge Assistant

A lightweight offline Knowledge Retrieval Assistant powered by FAISS and a local LLaMA 2 model. Built using FastAPI for the backend and Streamlit for the frontend.

---

## ğŸš€ Features

- ğŸ” Ask technical queries and retrieve relevant documents.
- ğŸ§  Embed documents with `SentenceTransformers` and index using FAISS.
- ğŸ¤– Run local inference using `llama-cpp-python`.
- âš¡ Minimal setup, fully offline, runs on CPU/GPU.
- ğŸ¯ Simple Streamlit UI frontend.

---

## ğŸ“ Project Structure


