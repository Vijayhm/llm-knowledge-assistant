# 🧠 LLM Knowledge Assistant

A lightweight offline Knowledge Retrieval Assistant powered by FAISS and a local LLaMA 2 model. Built using FastAPI for the backend and Streamlit for the frontend.

---

## 🚀 Features

- 🔍 Ask technical queries and retrieve relevant documents.
- 🧠 Embed documents with `SentenceTransformers` and index using FAISS.
- 🤖 Run local inference using `llama-cpp-python`.
- ⚡ Minimal setup, fully offline, runs on CPU/GPU.
- 🎯 Simple Streamlit UI frontend.

---

## 📁 Project Structure


